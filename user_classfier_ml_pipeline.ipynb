{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployable model for the Ad classifier dataset:\n",
    "\n",
    "def get_contain(x, word_list: list):\n",
    "    return any([keyword.casefold() in x for keyword in word_list])\n",
    "\n",
    "def if_influential_words(x, yes_words:list, no_words:list):\n",
    "    if any([keyword.casefold() in x for keyword in yes_words]):\n",
    "        return 1\n",
    "    elif any([keyword.casefold() in x for keyword in no_words]):\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def get_param_map(model):\n",
    "    param_grid_map = {'log_re': {\"C\": [0.01, 0.5, 1, 5, 10]}, \\\n",
    "                  'dec_tree': {'max_depth': [10, 20, 40],\n",
    "                              'min_samples_split': [2, 4, 8],\n",
    "                              'min_samples_leaf': [1, 3, 6]}, \\\n",
    "                  'rf': {'n_estimators': [250, 500, 1000],\n",
    "                          'max_features': [3, 4, 6],\n",
    "                          'max_depth': [10, 20, 40],\n",
    "                          'min_samples_split': [2, 4, 8],\n",
    "                          'min_samples_leaf': [1, 3, 6]}}\n",
    "    return param_grid_map[model]\n",
    "    \n",
    "def get_model_map(model):\n",
    "    model_map = {'log_re': LogisticRegression(random_state=42, penalty=\"l2\", solver='lbfgs'),\\\n",
    "                 'dec_tree': DecisionTreeClassifier(),\\\n",
    "                 'rf': RandomForestClassifier()}\n",
    "    return model_map[model]\n",
    "\n",
    "class Data_Scaler(object):\n",
    "    \n",
    "    def __init__(self, data_frame : pd.DataFrame, cols : list):\n",
    "        \"\"\"\n",
    "        Takes in the dataframe and the list with column names that need to be scaled\n",
    "        \"\"\"\n",
    "        self.data = data_frame[cols]\n",
    "        self.min_data = np.min(self.data)\n",
    "        self.max_data = np.max(self.data)\n",
    "        self.cols = cols\n",
    "    \n",
    "    # Scales down the values in the dataframe.\n",
    "    def transform_data(self, data_frame_to_scale):\n",
    "        data_frame_to_scale[self.cols] = (data_frame_to_scale[self.cols] - self.min_data)/(self.max_data-self.min_data)\n",
    "        return data_frame_to_scale\n",
    "    \n",
    "    # inverse scales the values in the dataframe\n",
    "    def inverse_transform_data(self, data_frame_to_inv):\n",
    "        data_frame_to_inv[self.cols] = data_frame_to_inv[self.cols]*(self.max_data-self.min_data)+self.min_data\n",
    "        return data_frame_to_inv\n",
    "    \n",
    "class Ad_Classifier():\n",
    "    \n",
    "    def __init__(self, param_grid = None, threshold=0.4, model = 'log_re'):\n",
    "\n",
    "        if param_grid == None:\n",
    "            self.param_grid = get_param_map(model)\n",
    "        else:\n",
    "            self.param_grid = param_grid\n",
    "        self.model = get_model_map(model)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"I'm a classifier model to predict if a user will click on the data or not. \\\n",
    "                I'm a Logistic regression model unless you specified otherwise. \\\n",
    "                Please don't give me null values. I'll get updated later to account for the null values.\"\n",
    "    \n",
    "    def pre_process(self, data):\n",
    "        data.dropna(axis=0, inplace=True)\n",
    "        data = data.drop_duplicates()\n",
    "        data = data[(data['Age'] >= 18) & (data['Age'] < 100)]\n",
    "        data = data[(data['Daily Internet Usage'] > data['Daily Time Spent on Site'])]\n",
    "        \n",
    "    def feature_engineering(self, data):\n",
    "        data['influential_words'] = data['Ad Topic Line'].apply(lambda x: \\\n",
    "                                                                if_influential_words(x, self.most_used_yes, self.most_used_no))\n",
    "        data.drop([\"Ad Topic Line\", \"Timestamp\", \"City\"], axis=1, inplace= True)\n",
    "               \n",
    "    def scaling(self):\n",
    "        numerical_vars = [\"Daily Time Spent on Site\", \"Area Income\", \"Daily Internet Usage\", \"Age\"]\n",
    "        self.scaler = Data_Scaler(self.data, numerical_vars) \n",
    "        self.data[numerical_vars] = self.scaler.transform_data(self.data[numerical_vars])\n",
    "    \n",
    "    def dummies(self, data):\n",
    "        return pd.get_dummies(data)\n",
    "        \n",
    "    def find_influential_words(self, data):\n",
    "        cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "        result  = [word for word in data[data[self.target]==1]['Ad Topic Line']]\n",
    "        yes_words = ' '.join(result)\n",
    "\n",
    "        result  = [word for word in data[data[self.target]==0]['Ad Topic Line']]\n",
    "        no_words = ' '.join(result)\n",
    "\n",
    "        yes_cloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, \\\n",
    "                             stopwords=cachedStopWords).generate(yes_words)        \n",
    "        no_cloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110, \\\n",
    "                             stopwords=cachedStopWords).generate(no_words)\n",
    "\n",
    "        self.most_used_no = set(islice(no_cloud.words_, 10))\n",
    "        self.most_used_yes = set(islice(yes_cloud.words_, 10))\n",
    "    \n",
    "    def process_train(self, data):\n",
    "        self.pre_process(data)\n",
    "        self.find_influential_words(data)\n",
    "        self.feature_engineering(data)\n",
    "        self.scaling()\n",
    "        return self.dummies(data)\n",
    "    \n",
    "    def process_test(self, data):\n",
    "        self.feature_engineering(data)\n",
    "        data = self.scaler.transform_data(data)\n",
    "        return self.dummies(data)\n",
    "    \n",
    "    def add_missing_dummy_columns(self, data):\n",
    "        missing_cols = set(self.columns) - set(data.columns)\n",
    "        for col in missing_cols:\n",
    "            data[col] = 0\n",
    "    \n",
    "    def fix_columns(self, data):  \n",
    "        self.add_missing_dummy_columns(data)\n",
    "        data = data[self.columns]\n",
    "        return data\n",
    "        \n",
    "    def validation(self):\n",
    "        grid_obj = GridSearchCV(self.model, param_grid=self.param_grid, cv=5)\n",
    "        grid_fit = grid_obj.fit(self.data.drop([target], axis=1), self.data[target])\n",
    "        return grid_fit.best_estimator_\n",
    "\n",
    "    def fit(self, data : pd.DataFrame, target : str, validation = True):\n",
    "        self.data = data.copy()\n",
    "        self.target = target\n",
    "        self.data = self.process_train(self.data)\n",
    "        self.model.fit(self.data.drop([target], axis=1), self.data[target])\n",
    "        if validation:\n",
    "            self.model = self.validation()\n",
    "        self.columns = self.data.drop([target], axis=1).columns\n",
    "        self.fitted_values = self.predict(data.drop([target], axis=1))\n",
    "        \n",
    "    def predict(self, data):\n",
    "        data = self.process_test(data.copy())\n",
    "        data = self.fix_columns(data)\n",
    "        return self.model.predict(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
